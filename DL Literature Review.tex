% !TeX encoding = UTF-8
\documentclass[a4paper]{article}
\usepackage{graphicx}
\usepackage[inline]{enumitem}
\usepackage{xcolor}
\usepackage[hidelinks]{hyperref}

\usepackage{todonotes}
\newcommand{\specialcell}[2][c]{%
		\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}	
%To disable notes:
%\usepackage[disable]{todonotes}
%opening
\author{Vasileios Lampridis	(1465093)}
\title{A practical review of Deep Learning methods}
\begin{document}
	
\tableofcontents
\listoffigures
\newpage

\maketitle
\listoftodos
\begin{abstract}
	%	This paper reviews the promising domain of Deep Machine Learning, with the purpose of providing the practical information about the field. It attempts to answer a number of  questions that a practitioner of Machine Learning could have, namely why Deep Learning superior is for some tasks and, thus, when this approach should be examined, what the main architectures in the field and the rationale for their design and which of them are usually used for a given task. 
	This work reviews the promising domain of Deep Machine Learning, with the purpose of providing the practical information about the field. It explains the limitations of shallow architectures, motivating the development of deep architectures.
\end{abstract}

\section{Introduction}
	There is no well-established definition of Deep Learning , but a brief one could be that it is a family of Machine Learning methods that aim to find a representation of data using consecutive non-linear transformations. Representation in this case is a vector of values, usually smaller than the original size of the input space, that can encode useful (statistical) information about the input. For example, images containing the same object(s) should be represented by similar values in the vector, even if the background is different. Words that have related use of meaning should be represented by similar values in their corresponding vector. It is this ability to capture such meaningful information, on types of input that is hard to achieve this, that sets Deep Learning methods apart and has raised the interest in the field.
	
	Deep Learning has raised a lot of expectations due to its successes in important domains, like computer vision. It has a quick adoption speed, with new ideas passing from the academia to the industry quicker than in other domains. Here lies a danger: The adoption of volatile methods, whose robust understanding is still under development could lead to false promises and a disappointment that could prove harmful for the reputation of this research field. This is not a new "threat", since in previous decades, undelivered expectations from Artificial Intelligence methods, like Neural Networks, led to decreased interest in the field, the so-called AI winters. The point, of course, is not to argue against the use of this very promising new technology but to highlight the importance of further understanding the fundamental theories, at a point in time where Deep Learning is becoming a buzzword outside of the academia and open-source libraries enable users to apply Deep Learning techniques without much background in the field. Thus, the purpose of this work is to attempt to transfer the extensive knowledge from studies about the benefits, methods and limitations of deep machine learning in a more compact form, with the aim of capturing the intuition around the theories and their practical implications.
	%that drives the content of this paper is that there could be a relative lack of concrete understanding from newcomers in the field that look to experiment with applying deep architectures to practical tasks. It can be useful
	
	% To give an example of such potential problems, it was recently found \cite{Nguyen2014} that Convolutional Neural Networks, a deep architecture for image clasification, are susceptible to reverse-engineering input images so that classified with high certainty by multiple such networks, even though the image can be incomprehensible to humans. This could mean that their use in an adversarial environment may not be a safe choice, .
	
	\paragraph{Outline}
		We explain the reasons why other Machine Learning methods fall short in a number of tasks and, thus, what are the cases where a Deep Learning method should be examined in \autoref{sec:Limitations}.	Basic concepts that are common in all representation learning methods are presented in \autoref{sec:Basic ideas}, with the focus of providing the theory that explains why deeper architectures have exceptional performance in some tasks.
		The three most popular deep architectures are briefly reviewed, Convolutional Neural Networks \ref{sec:CNN}, Deep Belief Networks \ref{sec:DBN} and Stacked Autoencoders \ref{sec:SAE}, attempting to explain the ideas that justify their design. Two important implementation ideas that improve the performance of such methods are presented, layer-wise pre-training \ref{par:Pre-training} and dropout \ref{par:Dropout}.
		Following, the most successful applications of Deep Learning are reviewed, in the domains of Computer Vision \ref{sec:CV} and Natural Language Processing along with Speech Recognition \ref{sec: NLP_Audio}.
		
	
\section{Limitations of shallow architectures}
\label{sec:Limitations}
	This section discusses what are the inherent limitations that exist in shallow architectures, that includes the more "traditional" linear and kernel-based Machine Learning algorithms. It provides a short explanation of what separates them from deeper architectures. It provides a background to understand the breakthrough that was achieved with use deeper architectures and the motivation for their exploration. Also, it is useful for a practitioner of Machine Learning to be able to evaluate whether it would 
	\subsection{Limitations of kernel-based learning.}
	\label{sub: kernel limitations}
		An enlightening review from Bengio and LeCun \cite{Bengio2007} provides a structured explanation of these kernel-based learning, from a theoretical perspective. Their claim is that methods that rely on local kernels may need an exponential number of parameters to approximate the function that relates the input to the output, if the function does not follow certain assumptions. This class of methods includes generally successful algorithms, like Support Vector Machines \cite{Boser1992,Cortes1995} and Gaussian Processes \cite{Williams2006}.
		Non-parametric and kernel based methods are shown to have an expected error that, for a given number of examples, rises exponentially to the number of the dimensions of the input space \cite{Hardle2004}. This kind of relation, common to many linear or local approximators is the so called curse of dimensionality.
		Expanding on this idea, kernel based methods make a trade-off between smoothness and locality \cite{Bengio2007}. Most kernel based methods have a hyper-parameter that tunes the area that they effect, like variance in Gaussian kernels. If the effective radius of a kernel is small, then a function with high variance can be approximated, but needs too many kernels to achieve it. If the kernels have a large effective radius the linear combination of them will be smooth and thus unable to represent accurately a function with high variance or curvature (though these two are related). In other words, the approximation of a point in most kernel based methods is a linear combination of the number of most dominant neighbours. Thus if a test example lies in a space where there are no close training examples, and the function is not smooth enough to be effectively extrapolated using more distant examples, the approximation will be inaccurate \cite{Bengio2013a}.
		
		It has to be highlighted that many practical problems do obey to the smoothness assumptions of kernel based models. Hence, they are effective one these problems and would probably be the most suitable method to use. Additionally, in order Deep Learning methods to be effective, there has to be some underlying pattern that if captured will provide a better approximation. If no such pattern exists, then no method should perform better than local learning \cite{Bengio2009}. Thus, Deep Learning is not a panacea. However, domains of great practical importance, like computer vision, are described by functions that do not seem to be smooth but have underlying patterns.
		
		Extending on the previous point, kernel-based methods need a large amount of parameters to approximate such functions. They may reach an acceptable accuracy, but this is expected to be at the cost of computational time \cite{Bengio2007}, both in training and testing time, due to the number of parameters they have. A deeper architecture can be faster, especially on the time needed for testing (new) samples, that gives a competitive edge when time is of essence for a task, for example, a continuous speech recognition system.
		
	\subsection{Engineered feature extraction}
	\label{sub: eng. features}
		One of the approaches to circumvent the limitations discussed previously is to simplify the input by applying a domain specific preprocessing. To have a comparison with the above, this is equivalent to designing non-local kernels that are based on prior knowledge about the task. The aims of this step are to reduce the dimensions of the input and create a (more) separable and smooth structure of the data so that, when presented to an approximator, it can be more accurate. The problem for this approach that the process is domain specific\cite{Bengio2007}. For example, a system designed to recognise car should not  be able to recognise animals. Moreover, the development of such methods relies heavily on the intuition around the problem and thus requires domain expertise and it is a slow trial and error process, which is costly in human capital. Before the use deep architectures, most top performing methods involved domain-specific preprocessing, yet, when retrospectively compared with these on common benchmarking datasets for audio and visual problems, this approach produces results of lower accuracy \cite{Krizhevsky2012}. 
		
		A pattern that has to be emphasised here is that the biggest successes of Deep Learning are on domains that we have limited intuition about the form of the function to be approximated, like tasks in vision, audio recognition and language. Arguably though, engineered feature extraction makes perfect sense in domains that we have explicit knowledge about the relation of the input and the output. For example,one could try and train a machine to retrieve the content out of an HTML page, but since it known exactly how HTML works, it is much easier and more effective to extract the content using an engineered method like a web scraping tool.
	
	In summary, shallow architectures and local learning algorithms seem to need an infeasible amount of parameters to approximate the structure of functions with high curvature and methods created to avoid this that are designed for one problem are not expected to be transferable in others. Thus, a number of points are good indicators that a deep architecture should be tried for a given task, a list that is by no means exhaustive: 
	\begin{itemize}
		\item There is limited understanding about the relation of the input and the output, that prevents us from engineering effective features,
		\item Relatively effective feature extraction methods exist for a specific task but are not transferable to similar tasks that may use the same input, while this would be desired.
		\item The relation between the input and the output is highly not linear and seems to have underlying regularities (patterns). Apart from domain knowledge, a hint about this could be that kernel-based methods, like SVMs, need a large number of parameters to reduce training error and/or fail to generalise to previously unseen inputs.
	\end{itemize}
\section{Basic ideas of Deep Learning}
	\label{sec:Basic ideas}
%	\paragraph{Promise of DL}
%		Before discussing the ideas around
%	Having discussed 
%	Benefits in quality and time
%	Enabling new applications (multi-modal)
%	Transferable algorithms
%	Big Data
	\paragraph{Setbacks in investigation of deeper models}
		Since the use of the above methods has the aforementioned limitations yet they were the dominant approach, the natural question is what hindered the development of more suitable methods. Before introducing the approach of deep architectures, it would be useful to discuss the main difficulties that kept the research away from a more extensive investigation of this approach.
		
		The main problem faced when trying to train Neural Networks with many hidden layers is that during the training they exhibit the problem of the so called vanishing or exploding gradient that leads to poor accuracy. During the training process, the gradient that changes the weights of the lower layers gets very small or sometimes very large, preventing the effective training of these layers, which leads to a low accuracy of the Neural Network \cite{Bengio2009} This problem has been studied, with multiple possible explanations provided, beyond the current scope, discussed in \cite{Bengio2007}. The result of the above problem is that shallower architectures, due to their easier training, were outperforming deeper architectures. This is despite the fact that a network with n+1 layers can have in theory all the expressive power of a network with n layers, implying that it is the training method that is problematic rather than the architecture. This obstacle was overtaken by a greedy layer-wise training method, that could be said that marks the beginning of Deep Learning, and is explained at \ref{par:Pre-training}. It has to be mentioned though that Convolutional Neural Networks \ref{sec:CNN}, probably due their specific, constrained architecture, were able to have multiple layers, yet were trained without the need for aforementioned method.
	
	\paragraph{Distributed Representations}
		The representations of the input that is learned by all deep architectures are distributed. This concept can be easily explained when compared with local kernel based methods like clustering algorithms, Gaussian mixtures, decision trees and others. These methods have a mutually exclusive type of representation. For example a sample can be  member of only one cluster and so on. This means that they need $O(N)$ parameters (eg. clusters) to distinguish between $O(N)$ examples. In contrary, distributed representations are using multiple parameters to represent a point in the input space. Because the number of combinations is exponential to the number of parameters, they can distinguish between up to $O(2^N)$ examples. A review on representation learning \cite{Bengio2013c} provides an excellent explanation of this concept.
		
		It has to be highlighted that distributed representations are created by other methods, like Principal Components Analysis or Independent Components Analysis. However, these methods create a linear transformation of the input. The implication of this is that if we stack these methods, the resulting representation is still a linear transformation of input space \cite{Bengio2007} and stacking them does not lead to learning more complex features. Thus, deep learning methods enjoy the expressive power of distributed representations, but also create a highly non-linear transformation of the input.
		
		A very naive way to create a smaller representation of the input could be to randomly ignore 10\% of the input variables. The representation would preserve 90\% of the information, but it would not be of any value, because this transformation does not offer any information about the factors that explain the variations in the input. If we would like to represent different sinusoidal curves generated by $ f(x)=a*sin(b*x)$ then the ideal representation would need only two parameters describe it $a, b$, while a naive would need a great number of points to interpolate between this varying function. Thus, a compact representation is not enough to be distributed, but should reveal the underlying factors\cite{Bengio2013c}.	One of the founding studies of the field \cite{Hinton2006a} performs a similar experiment with the above, where a Deep Belief Network learns to represent many different generated curves with only a few factors, that can be seen as reverse-engineering the generating function of these curves. 
		
		When such underlying factors are discovered, complex classifications are made easy. In the previous example, the sinusoid curves can be classified based of their frequency/curvature very easily, just by knowing the value of $b$, a task that would be much more difficult without discovering the underlying pattern.
		In simpler problems, uncorrelated principal components that are found by PCA can indeed provide a good explanation of the factors of variations in the data. In highly non-linear spaces, though, the non-linear transformation of deep architectures outperform them by learning features that are quantitatively and qualitatively better \cite{Hinton2006}.	

	\section{Deep Architectures}
		Since the field is young, many different architectures and variations have been tried. Out of these, three stand out as the most successful and well-researched: 1) Convolutional Neural Networks, 2) Deep Belief Networks and 3) Stacked Autoencoders. \cite{Bengio2012} provides an excellent review that focuses on these three architectures.
		
		\subsection{Convolutional Neural Networks}
		\label{sec:CNN}			
		The architecture that dominates the (purely) supervised methods in this domain is Convolutional Neural Networks. It is an architecture initially inspired by models of the visual cortex of the brain \cite{Fukushima1980}, and is,thus, designed using simple priors about the structure of visual information (images) \cite{Bengio2013c}. It is unique in the family of deep architectures in the sense that it does not need a layer-wise pre-training to converge to accurate solutions. It has acquired great attention due to the large error reduction that variations of it have achieved in recognised computer vision challenges like ImageNet \cite{Deng2009a}. 
		
		Excellent descriptions provided of older implementations are provided in \cite{LeCun1989,LeCun1990,Lecun1995,LeCun1998}, which convey well the most important ideas around their design. Later implementations, that achieved top performance in image classification tasks, are explained in \cite{Krizhevsky2012}. The main ideas that improved the performance and accuracy of these methods are the use of dropout \ref{par:Dropout} as a regularisation method and the use of Rectified Linear Units (ReLU) instead of the usual sigmoid functions used in most Neural Networks.
		\todo {restructure CNN section, and CITATIONS!}
		
		Compared to other deep architectures, Convolutional Neural Networks have the most straight-forward training process. The network is trained in a supervised way by back-propagating errors. This explains the fact that they were used even before the innovation of the layer-wise greedy -ore-training that was critical for the effective training of other architectures.	
		
		
		The basic ideas around their design are listed, along with the assumptions that their choice imply, with the aim of explaining the design choices of a Convolutional Neural Network, rather than its functionality.
		\begin{itemize}
			\item Similarly with other deep architectures, they have successive layers of similar functionality that aims to learn progressively more complex and specific features, with the last layer representing categories. 
			\item There is no distinct feature extraction module and classification module in their structure. This means that their training procedure does not have an unsupervised pre-training. The internal representation of the input is implicitly learned, driven by the supervised training.
			\item Every layer gets as input a two dimensional array, and all operations are done in patches of this input. This exploits the prior that pixels that are close carry related information.
			\item There is an alternation of two types of layers with different roles. Thus each new level of abstraction is be achieved by a couple of these layers. These are 1) the convolution and 2) the subsampling or pooling layer. In a convolution layer there are multiple non-linear transformations of the input (image or feature matrix), each one creating a different output matrix. In a subsampling layer each of the outputs of the previous layer is down-sampled  by a procedure over non-overlapping regions of the input. This procedure is usually an averaging or maxing of each region that creates a single output. Since each point of the output is created by a region of the input and these regions don't overlap, a down-sampling occurs.
			\item Because of the consecutive averaging of maxing procedures in each layers, the output becomes increasingly tolerant , in other words invariant, to small variations of the input. A tolerance to such changes is one of the core challenges in object recognition.
			\item Closely related to the above, the receptive field of each successive layer grows larger, because of the down-sampling. The higher the layer, the larger the patch of the original image that affects the value of a "pixel" in this layer.  Thus, convolution extracts features, while pooling recombines the extracted information in a more meaningful way. Together they extract specific features over the whole the image.
			\item There are shared weights in each layer. In each layer, the weights are identically applied to all parts of the input. That is what explains the name of CNN, since this means that the 2-dimensional input is \textit{convolved} with a small matrix of weights to produce the output of the layer, acting like a filter. This is exploiting the assumption that a filter that is useful in one part of the image is probably useful in other parts of it. This design choice vastly decreases the number of trainable parameters and is the most probable explanation for the comparably easy training of the network.
		\end{itemize}
		
		\subsection{Deep Belief Networks}
		\label{sec:DBN}		
			This architecture is the one that initiated the interest in deep architectures, since it proved capable of having superior performance at vision and Natural Language Processing tasks \cite{Hinton2006,Hinton2006a}. 

			It is a generative graphical model that is built by multiple layers of stochastic, latent (hidden) variables. There are connections between the layers, but not between the units of each layer. They are trained using a greedy method that trains each layer separately \cite{Hinton2006a}. 
			
			The Restricted Boltzmann Machines are the building blocks of this architecture. These are an undirected graphical model with one visible layer, one hidden layer of stochastic variables, and connections existing only between these two layers, not inside them, this property, that justifies the "restricted" in their name, makes the variables of the hidden layer conditionally independent given an input. The exact learning procedure is still computationally inefficient (despite the restriction) but can be efficiently approximated by a simple three-step process that minimises a quantity called Contrastive Divergence \cite{Hinton2002}. This method is the workhorse of the training procedure of the Restricted Boltzmann Machines and, consequently, of Deep Belief Networks. Since Restricted Boltzmann Machines are undirected, the can also act generatively and produce what can be seen as an expected input.
			
			Deep Belief Networks are created stacking multiple Restricted Boltzmann Machines. This is done by having the hidden layer of one Restricted Boltzmann Machine to be the visible layer (thus the input) for the next one. The training is done layer-by-layer,  with the variables of each layer learning how they depend to the variable of the above layer. When all layers have been trained, they can be jointly fine-tuned. If the task is supervised, then an extra layer containing the expected output can be set on top to back-propagate error derivatives to increase the discriminative accuracy of the network \cite{Hinton2006}. If used generatively, they can create random "imaginary" inputs (ex. images) given an output.
				
			Summarising, this is an unsupervised method that can learn multiple consecutive transformations of an input that can greatly assist both supervised and unsupervised tasks. They can also be used to generate inputs similar to the inputs it was trained with. They are created by stacking Restricted Boltzmann Machines that are trained one-by-one using Contrastive Divergence.
			
		\subsection{Stacked Autoencoders}
		\label{sec:SAE}		
			Autoencoders are a family of unsupervised methods that have the goal of finding a generalisable transformation of the input, they can be seen as feature extractors. They are usually used to assist a classifier in a supervised task, to visualise high-dimensional data or to find clusters and to create similarity metrics between samples.
			They can be seen as consisting of two functions: an encoder $f(x)$ and a decoder $g(x)$. The encoder transforms an input  and the decoder reconstructs the transformation to the original input. In all variations of autoencoders, these to functions are jointly trained together, with the goal of minimising the reconstruction error $r(x)=g(f(x)), err(x)=\|x-r(x) \| $. Of course, the easiest way to do this is using two identity functions, which would have a zero reconstruction error, but it would not offer any value. Hence, another objective is needed to avoid this: the encoder should reconstruct well only for inputs similar to the training set. There are four main approaches to achieve this regularisation \cite{Bengio2013c}, that are explained below: 
			\begin{enumerate}
				\item Make the dimensions of the output fewer than the input (create bottleneck)
				\item Restrict the output to be sparse
				\item Add random noise to the input, requiring a reconstruction of the original input. (Denoising)
				\item Analytically restrict the sensitivity to input variations (Contractive autoencoders)
			\end{enumerate}
			In many cases, it is desirable to reduce the dimensionality of the input, having a reduction of dimensions is avoiding the identity function without any explicit changes to the system. However, it was found \cite{Bengio2013c} that explicit regularisation gives smaller reconstruction errors and qualitatively better features. Actually, over-complete transformations, meaning that the output has higher dimension than the input, are used in some tasks and are only possible by explicit regularisation.	
			The first approach to training autoencoders was to impose a sparsity constraint \cite{Bengio2007a,Ranzato2007}, where the output of hidden neurons is penalised, encouraging them to have values closer to zero. 
			A better method to regularise autoencoders is to add noise to the training example at each step and expect the original (denoised) input as the output of the autoencoder. This method forces the system to learn the structure of the input distribution, so that it can undo the effects of the corruption (due to the added noise). 
			The effect of learning to undo the noise is that the system is more robust to small changes of the input. In order to have this, the learned features of the autoencoder have to be tolerant to the changes of most values in the input space. This objective can also be analytically captured by requesting the derivatives  of each unit given the input $ \frac{\partial h_i(x)}{\partial x} $ to be small.
			
			
		\subsection{Similarities and differences}
			All deep architectures have the goal of building a non-linear, distributed representation of the input. Deep Belief Networks and Stacked Autoencoders have this goal explicitly set as their training objective. Convolutional Neural Networks achieve this implicitly, since, although they are trained for classification, the consecutive filtering and pooling transformations do capture an effective lower-dimensional description of the input at the last hidden layer of the network \todo{cite UL CNN}.
			
			Since both Deep Belief Networks and Stacked Autoencoders are functionally similar, it is worth trying to compare them. Indeed, they can be used interchangeably for a task, meaning that they can be set to receive the same input and ultimately produce the same form of output (ex. a vector of features). There is probably no easy answer on how to decide which is more effective for a given task.
					
		The following tables summarises the basic differences between the three architectures.
		\begin{table}
			\begin{tabular}{lllll}
				Arch. & Learning & Training methods & Domains \\
				CNN & supervised & Back-Propagation  & Vision  \\
				DBN & unsupervised & \specialcell{Layer-wise pre-trainng, \\ Contrastive Divergence}  & Multiple Domains \\
				SAE & unsupervised & \specialcell{Layer-wise pre-trainng,\\ Back-Propagation} & Multiple Domains \\
			\end{tabular}
		\end{table}
\section{Notable ideas on implementing Deep Learning methods}
	\subsection{Layer-wise pre-training}
	\label{par:Pre-training}
		A method that is also mentioned in description of specific architectures is the initial separate training of each layer of a layered architecture to get good initial weights for each layer \cite{Hinton2006a,Bengio2007a}. The idea is that each layer, that is practically a smaller approximator, can be trained separately to learn a more accurate representation of their input, which can be the real input or the output of a hidden layer. After having learned some useful initial weights for each layer using this fast and greedy method, the whole network can be trained, to further tune these initial weights. 
		There have been to main hypotheses about why this method is helping: 1) It improves optimisation (reduces both training and testing error) 2) It improves regularisation (reduces testing error). Different studies can confirm both hypotheses \cite{Bengio2009}. It is found that networks trained without no such pre-training can have very low training error, yet high testing error, which points mostly in the direction of regularisation. The conclusion in any case is that this pre-training, that proved to be the key in training many kinds of deep architectures helps the network to learn better representations of the input. It helps to think that it seems easier to make, for example, a network to learn edge detectors, then corner detectors, then more complex features, which lead to class-specific features like face detectors in a serial way, even if it is greedy, than to learn them all at the same time.
	
	\subsection{Dropout, a regularisation method}
	\label{par:Dropout}
		Deep Networks have many parameters to be learned and are prone to overfitting. Consequently, a constraint has to be used during the training to avoid this. $L_1, L_2$ regularisations have been used, but the recent method of dropout appears to have much better effect, independently of the domain or the network that is applied \cite{Srivastava2014}.
		
		Dropout is a powerful regularisation method, in which the weights of some randomly chosen units of the network,  (half of the total units of hidden layers was found to be optimal), are temporarily set to zero at each training step \cite{Srivastava2014,Dahl2013}. These units do not contribute for the given computation and can be seen as dropped from the network. When the network is used for test examples, all the units of the network contribute, with their weights multiplied by the probability that the unit was present in the network. If half were being dropped, then the learned weights are multiplied by 1/2.
		Dropout make the subject network to have different, randomly chosen units missing from each layer at each training step. This can be interpreted as a different network at each step, that shares weights with the other randomly created ones at previous steps. Then, during testing, all the different networks are averaged to give the output. Thus, dropout bears similarities with the older ideas of boosting and bagging. Dropout can also be seen as extending the previously discussed idea of Denoising Autoencoders \ref{sec:SAE}. An approach to adding noise in these autoencoders is to set some units of the unit to zero, thus dropout is the extension of this idea to hidden layers.	
\section{Domains of applications}
\label{sec:Domains}
	The aim of this section is to present the most popular deep learning approaches to a number of domains and provide, whenever possible, some intuition about the reasons each architecture is used at each domain. The domains that will be discussed are:
	\begin{enumerate*}
		\item Computer Vision
		\item Natural Language Processing and Speech Recognition
	\end{enumerate*}.	
	In Computer Vision, the most popular and researched task is Image Classification, with a smaller interest in object detection. In Audio Recognition, the most interesting task is speech recognition. Natural Language Processing contains a family of tasks revolving around reasoning with written language, like machine translation, sentiment analysis, document querying and others. 
	The domain problems are briefly described, along with the attributes of them that are relevant to the development of Machine Learning architectures. Then the most common deep architectures for each domain are briefly presented, with an informal discussion about possible reasons they are preferable for the given domain. 
	\subsection{Computer Vision}
	\label{sec:CV}
		The general problem in this field is could be described by the question "how to reason about the contents of an image". 
		The most common and well studied problem from a machine learning perspective is to classify the image, that is to assign one or more labels from a given set, that are usually meant to describe the contents of the image, for example "face", "car", "1". This is arguably the most studied problem in the deep learning literature.
		Other common problems are image segmentation and object detection.
		
		There are some inherent challenges in object recognition. The output should be independent of variations in the input image, like translation, rotation, scaling, stretching and other distortions of the image. Also, objects may be partially obstructed by others, and have a noisy background. To give an example of these challenges, humans can recognise an upside-down car in any (visible) scale and in any position on an image, even if some part of it is hidden and the car is in a junk-yard (a noisy background). An ideal object recognition system should be able to match these abilities of the human visual cortex, achieving an invariance to input distortions. 
		
		There are two main approaches to object recognition in the Deep Learning literature. The former is using unsupervised methods to extract a more compact representation of the input. The other is using supervised training of the whole architecture at the same time, although minor improvements to accuracy have been achieved with some form of unsupervised pre-training.
		\paragraph{Supervised methods}
			Supervised learning in image classification is dominated by the use of Convolutional Neural Networks \ref{sec:CNN}
			\todo{Add smth on vision}
		\paragraph{Unsupervised and semi-supervised methods}
			\todo{Check UL part on vision, add citations}
			% Focus on the thesis!! Provide guidlines on how this and why!
			Another popular approach to image classification, especially before the domination of the field by Convolutional Neural Networks in the recent years, was to have a an architecture with separate feature extraction and classification. The feature extraction module is trained in unsupervised way in order to learn a compact and invariant representation of the input. Then supervised training is use to jointly train both modules, where the unsupervised module is calibrated for a more discriminative representation along with the training of the classifier of choice.
			
			The deep architectures that are used for the feature extraction are falling into the category of autoencoders. These methods have the goal of finding a model that can reproduce the output of multiple samples with the minimum reconstruction error. Since the same model is reconstructing multiple samples, some statistical relationship between the input variables for the given set of samples has to be captured. Thus, they can generalise to previously unseen samples.
			They are usually designed to have a bottleneck in their architecture, with considerably fewer variables than the input. Since the input to be reconstructed is represented with these fewer variables, a dimensionality reduction is achieved, in a was keeps the maximum information about the input, so that it can be reconstructed as accurately as possible.
			
			These methods can be applied in many domains and studies used models that are naive about the structure of the input, meaning that the models weren't exploiting in any way prior knowledge about the structure of images. For example, if the pixels of all images were to be permuted in the same way, it should not affect performance of the models.
			
	\subsection{Natural Language Processing and Speech Recognition}
	\label{sec: NLP_Audio}
		Natural Language Processing is a broad field that involves manipulating knowledge about language to tackle tasks to involve reasoning with the contents of words, phrases and sentences. There is a much larger variety of tasks that are relevant to this field. This is indeed expected, since using natural language is the most common way of encoding human knowledge and reasoning with it. 
		Some popular applications of Deep Learning are on Sentiment Analysis, Semantic similarities of documents, Machine Translation,Named Entity Extraction. Since managing and reasoning about the world's information is a large industry, algorithms that can improve the process in terms of quality or speed can have an impact on this industry. Thus, algorithms in this domain are promising and worth of further exploration.
		
		In all of this tasks, the common theme is that an automatically learned distributed representation of words is used, in the form of a vector, a method called word embedding \cite{Collobert2008}. This is in contrast with the traditional approach in Natural Language Processing, where the words are encoded in a (very) sparse vector, a method called Bag of Words.Depending on the training process involved in learning the word representation, similarities and other relations such as analogies can be discovered. Most approaches rely on multilayer Neural Networks to learn these embeddings \cite{Collobert2008,Collobert 2011}. They are either feed-forward or recurrent models. Recurrent Neural Networks are  also tested, and are a logical choice since they can capture the serial relation between words, or even letters. 
		 
		Word embedding is a great example of the expressive power of distributed representations\cite{Collobert 2008}: Since a word is encoded by a combination of elements in a vector, relations between words are much easier to be expressed. For example, a word that is similar to two words that are dissimilar between them, can share elements of the vector with both, while the others do not share any.   Thus complex relations between words can be captured.For example, "crane" is related to both birds and construction machinery words, but other words about birds and machinery are unrelated. In fact, since the combinations of elements are exponential to the size of a vector, an exponential number of interactions can be potentially expressed. This intuitively justifies the choice of a distributed representation to encode entities with complex relations between them.
		A recent find gives an impressive insight on the representation obtained: Analogies are autonomously learned, and can be discovered by subtracting the vectors that represent each word \cite{Mikolov2013}. For example $f("king")-f("queen") \approx f("man")-f("woman") $, where f is the embedding function.
		
		The first related work was on finding semantic similarities of documents in an unsupervised way. This can be used to retrieve similar documents given a new one, or to reduce the dimensions of the learned space to plot the documents as points and discover clusters \cite{Hinton2006,Salakhutdinov2009a}. A very interesting expansion of word embedding is bilingual embeddings, where words from two languages are embedded in the same space. This allows interesting applications, with the most useful being machine translation \cite{Devlin2014}. Another field where these approaches prove useful is text sentiment analysis, where the sentiment of a piece of text is being evaluated. Interesting findings can be found in the following works, \cite{Le2014,Socher2011}. The identification of the syntactic role of words  called part-of-speech tagging has been also assisted by a word embeddings \cite{Santos2014}. A recent study presented a deep learning approach to the hard problem of understanding more subtle relations like the context and structure of a text \cite{Ji2014}. This work follows the common theme, where Deep Learning methods outperform engineered features and local learning methods. However, the improvement is smaller when compared to the breakthroughs in vision and audio tasks \cite{Deng2014}.
		
		A task that is related to the above domain is Speech recognition. This task has two main components, the extraction of information from sound and the transformation of these sound features to text. The latter is obviously related to Natural Language Processing problems, since knowledge about language can greatly assist the predicted text. \todo{Throw some refs on speech recogntion}
		\todo{Sumarise NLP}	
	
	
%\section{Remaining challenges}
%	\todo{Write for challenges}
%	This section discusses the remaining challenges in this field. Interestingly, many of them are very closely related to the technological implementation of Deep Learning methods.
%	\paragraph{Scaling up}
%		It has been shown \cite{Le2011c} that large-scale training, using models with millions of parameters, trained on a vast number of examples can be beneficial for Deep Learning methods, allowing them to learn higher level features.
%	\paragraph{Distributed training}
%	\paragraph{Optimisation}
%	\paragraph{Progressing the theory and understanding around Deep Learning}	
\section{Conclusion}
	% Prove the thesis!!
		% This paper reviews the promising domain of Deep Machine Learning, with the purpose of providing the practical information about the field. It attempts to provide an number of common questions that a practitioner of Machine Learning could have, namely 
%		why is Deep Learning superior for some tasks and, thus, 
%		when this approach should be examined, 
%		what are the main architectures in the field and 
%		the rationale for their design and 
%		which of them are usually used for a given task. 
	In order to explain the motivation behind the development of representation learning, this work initially presented the shortcomings of kernel-based methods \ref{sub: kernel limitations}, which practically include the most successful alternatives to representation learning methods. The weaknesses of engineered feature extraction \ref{sub: eng. features} are then highlighted, concluding to a number of indicators of the potential need for a better alternative. Following, the
	
	
	Following, an brief description of the of the three most popular architectures is provided: Convolutional Neural Networks \ref{sec:CNN}, Deep Belief Networks \ref{sec:DBN} and Stacked Autoencoders \ref{sec:SAE}. The overview is focusing on the concepts that explain the design and use of each architecture, with references to the original research for details about the actual implementation. \todo{Conclusion about each architecture}
	A overview of the uses of Deep Learning methods in different domains provides practical examples of the previously presented concepts and architectures. It also 
	\todo{Conclusion about each domain}

\bibliographystyle{plain}	
\bibliography{library}	


\end{document}
